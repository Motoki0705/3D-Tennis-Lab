# @package _global_

task: train # 'train' or 'infer'

# Experiment name for logging
experiment_name: "ball_sequence_predictor"

# --- Data Config ---
data:
  # Path to the root directory of the images (not used for this model, but good practice)
  images_root: data/processed/ball/images
  # Path to the annotation file
  labeled_json: data/processed/ball/annotation.json

  # Sequence parameters
  sequence_length: 50 # Number of frames in the input sequence
  predict_offset: 1 # Predict N frames into the future

  # Splitting
  val_ratio: 0.2
  split_seed: 42

  # Dataloader params
  batch_size: 64
  num_workers: 4

# --- Model Config ---
model:
  type: transformer # lstm | gru | transformer
  # [x, y, vx, vy, ax, ay]
  input_dim: 6
  hidden_dim: 128 # for lstm/gru
  n_layers: 2
  dropout: 0.1
  bidirectional: false
  # transformer-specific
  d_model: 128
  nhead: 4
  dim_feedforward: 256
  output_dim: 6 # Predicting the next state vector

# --- Training Config ---
training:
  # Trainer settings
  max_epochs: 100
  accelerator: auto # auto, cpu, cuda, mps
  devices: 1
  precision: 32 # 16 or 32
  gradient_clip_val: 0.5

  # Optimizer
  lr: 1e-3
  weight_decay: 1e-5
  betas: [0.9, 0.999]

  # LR Scheduler (Cosine Annealing with Warmup)
  warmup_epochs: 5
  eta_min: 1e-6

  # Prediction mode & losses
  predict_mode: absolute # absolute | delta
  consistency_loss_weight: 0.1 # physics-inspired consistency loss weight
  loss_mode: "xy_only" # "full_vector" or "xy_only"

  # Loss function (currently fixed to MSE in lit_module)

# --- Callbacks (placeholders) ---
callbacks:
  checkpoint:
    monitor: "val_loss"
    mode: "min"
    save_top_k: 3
    filename: "epoch={epoch}-valloss={val_loss:.4f}"

  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: "epoch"

  visualization:
    _target_: development.ball_tracking.ball_tracker.callbacks.visualization.VisualizationCallback
    plot_interval: 10 # Log every N validation epochs
    plot_samples: 5 # Number of samples to plot per visualization
  # early stopping (optional override by user)
  # early_stopping:
  #   monitor: "val/loss"
  #   mode: "min"
  #   patience: 10

# --- Inference Config ---
inference:
  # Path to the model checkpoint (.ckpt) to load. If 'best', finds the best checkpoint automatically.
  checkpoint_path: "best"
  # Dataset split to use for inference ('val', 'train', or 'all')
  dataset_split: "val"
  # Output path for the inference results (e.g., a JSON file)
  output_path: "outputs/inference_results.json"
  # Number of samples to run inference on (null for all)
  limit_samples: null
